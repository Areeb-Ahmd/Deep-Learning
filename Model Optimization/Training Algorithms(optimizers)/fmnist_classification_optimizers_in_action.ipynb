{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4093aaf",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Optimizers in Action with Fashion MNIST Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7b3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e9cd3",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f58cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0acfd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a993a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47581288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pullover'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "classes[labels[7].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1532ad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(images[7].cpu().squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e24cf1",
   "metadata": {},
   "source": [
    "### Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da69d039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "else \"mps\" if torch.backends.mps.is_available()\n",
    "else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1d6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClothsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e0d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClothsClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb69cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 2.3185243606567383\n",
      "Batch: 100, Loss: 0.8023175597190857\n",
      "Batch: 200, Loss: 0.4705267548561096\n",
      "Batch: 300, Loss: 0.6208277940750122\n",
      "Batch: 400, Loss: 0.5188816785812378\n",
      "Batch: 500, Loss: 0.4613702893257141\n",
      "Batch: 600, Loss: 0.43556636571884155\n",
      "Batch: 700, Loss: 0.5920909643173218\n",
      "Batch: 800, Loss: 0.5758353471755981\n",
      "Batch: 900, Loss: 0.48105713725090027\n",
      "Batch: 0, Loss: 0.30124232172966003\n",
      "Batch: 100, Loss: 0.4285827875137329\n",
      "Batch: 200, Loss: 0.31610047817230225\n",
      "Batch: 300, Loss: 0.43879061937332153\n",
      "Batch: 400, Loss: 0.36470791697502136\n",
      "Batch: 500, Loss: 0.3693581223487854\n",
      "Batch: 600, Loss: 0.36225903034210205\n",
      "Batch: 700, Loss: 0.5159111022949219\n",
      "Batch: 800, Loss: 0.48690474033355713\n",
      "Batch: 900, Loss: 0.45995524525642395\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = loss_fn(pred, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch: {batch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6accf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Append labels and predictions to lists\n",
    "        all_labels.extend(labels.cpu().numpy())    \n",
    "        all_predicted.extend(predicted.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2886010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81      1000\n",
      "           1       0.96      0.97      0.96      1000\n",
      "           2       0.83      0.63      0.72      1000\n",
      "           3       0.86      0.87      0.86      1000\n",
      "           4       0.64      0.89      0.74      1000\n",
      "           5       0.98      0.93      0.95      1000\n",
      "           6       0.73      0.54      0.62      1000\n",
      "           7       0.89      0.97      0.93      1000\n",
      "           8       0.95      0.97      0.96      1000\n",
      "           9       0.96      0.93      0.95      1000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.86      0.85      0.85     10000\n",
      "weighted avg       0.86      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_labels, all_predicted)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
